# ARNet-v2
### Assisted Refinement Network Based on Channel Information Interaction for Camouflaged and Salient Object Detection

**Authors:** Kuan Wang, Yanjun Qin, Meng-ge Lu, Liejun Wang and Xiaoming Tao

[![arXiv]([https://img.shields.io/badge/arXiv-2512.11369-b31b1b.svg)](https://arxiv.org/abs/2512.11369](https://arxiv.org/abs/2512.11369))
---

Camouflaged Object Detection (COD) remains a challenging task in computer vision, aiming to identify and segment objects that are visually integrated with their surroundings. Although existing methods have achieved progress in cross-layer feature fusion, two key issues persist during decoding:
(1) insufficient cross-channel information interaction within same-layer features, which limits feature expressiveness, and
(2) ineffective collaborative modeling between boundary and region information, leading to incomplete regions and inaccurate boundaries.

To address these challenges, we propose the Assisted Refinement Network (ARNet) â€” a dual-dimensional collaborative decoding framework that enhances both feature interaction and boundaryâ€“region consistency.

### Key Contributions

Channel Information Interaction Module (CIIM)
Introduces a bidirectional horizontalâ€“vertical integration mechanism at the channel dimension, enabling feature reorganization and interaction across channels to capture complementary cross-channel information effectively.

Boundary Extraction (BE) and Region Extraction (RE) Modules
Generate boundary priors and object localization maps, respectively, and employ Hybrid Guided Attention (HGA) within CIIM to jointly calibrate decoded features, thereby improving boundary sharpness and spatial localization.

Multi-scale Enhancement (MSE) Module
Enriches contextual feature representations using a three-branch multi-scale convolution strategy, expanding the receptive field and improving globalâ€“local feature integration.

### Experimental

Extensive experiments on four COD benchmark datasets demonstrate ARNetâ€™s state-of-the-art performance and strong generalization ability.
We further transfer ARNet to the Salient Object Detection (SOD) task and validate its adaptability across various downstream applications, including polyp segmentation, transparent object detection, and industrial/road defect detection.

---

## ğŸ“– Table of Contents
- [Our Motivation](#-our-motivation)
- [Overview](#-overview)
- [Environment](#-environment)
- [Materials & Evaluation](#%EF%B8%8F-materials--evaluation)
- [Experimental Results](#-experimental-results)

---

## ğŸ¯ Our Motivation

<img width="1720" height="801" alt="image" src="https://github.com/user-attachments/assets/964c73a3-28f0-43bd-89d9-45985a2030d4" />


---

## ğŸ“ Overview


<img width="1584" height="826" alt="image" src="https://github.com/user-attachments/assets/b3cdf857-0aba-4436-8864-a99a5a7624fd" />


<img width="1615" height="529" alt="image" src="https://github.com/user-attachments/assets/a7b2a482-7edf-46a8-b735-532c155eda9d" />


---

## ğŸ’» Environment

-   `python = 3.9`
-   Other packages can be found in `requirements.txt`.

---

## ğŸ› ï¸ Materials & Evaluation

### Required Materials
+   **Datasets:** You can find the [training and test datasets](https://github.com/DengPingFan/SINet/) here.
+   **SMT Weights:** Download from [Google Drive](https://drive.google.com/file/d/1F8E_Ca6nvusNjp0SqWBVyImWsUSMTSN1/view?usp=sharing).
+   **Pre-trained Model:** Download our weights from [Google Drive](https://drive.google.com/file/d/18xlY7ZtTwPk7MCdYnGMIpj1hafc0GZBq/view?usp=sharing).
+   **Prediction Results:**
    +   [COD Results (Google Drive)](https://drive.google.com/file/d/1mLujes7kTj_6BZrdvfTEGhk4EzN6dttR/view?usp=sharing)
    +   [SOD Results (Google Drive)](https://drive.google.com/file/d/1yM52z75vYh058-c00wG7dfgtd8-2oVpL/view?usp=sharing)
    +   [Polyp Results (Google Drive)](https://drive.google.com/file/d/1jY4_BzgSZzjrtztGtZt9_PEIqJaFfO0L/view?usp=sharing)

### Evaluation Script
We provide a simple script to reproduce the quantitative results.

#### ğŸ“‚ File Structure
Please organize your files as follows to run the script without any modifications:

```
.
â”œâ”€â”€ eval
â”œâ”€â”€ evaluation
â”‚   â””â”€â”€ evaluation.py
â”œâ”€â”€ TestDataset
â”‚   â””â”€â”€ NC4K
â”‚       â””â”€â”€ GT
â”‚           â”œâ”€â”€ <mask_1>.png
â”‚           â”œâ”€â”€ <mask_2>.png
â”‚           â””â”€â”€ ...
â””â”€â”€ test_maps
    â””â”€â”€ ARNet
        â””â”€â”€ NC4K
            â”œâ”€â”€ <pred_1>.png
            â”œâ”€â”€ <pred_2>.png
            â””â”€â”€ ...
```
-   `TestDataset/NC4K/GT/`: Contains the ground truth masks for the NC4K dataset.
-   `test_maps/ARNet/NC4K/`: Contains the predicted maps from our method (ARNet) on the NC4K dataset.

#### ğŸš€ How to Run
Simply execute the `evaluation.py` script to automatically compute and display all evaluation metrics.

```bash
python evaluation.py
```
Note: You can also modify the mask_root and pred_root variables within the script to evaluate different datasets or methods.

## ğŸ“Š Experimental Results

<img width="1115" height="817" alt="image" src="https://github.com/user-attachments/assets/00e83bf4-bb61-4b46-9983-e9e2232ade8a" />


![COD1](https://github.com/user-attachments/assets/c38d73d4-2902-48aa-a37a-2a8de80a3f32)



